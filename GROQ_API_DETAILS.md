# Groq API - Complete Reference

## ğŸ¯ **Overview**

Groq provides ultra-fast LLM inference with **OpenAI-compatible API** format, making migration straightforward.

---

## ğŸ“¡ **API Compatibility**

### **OpenAI Compatible**
```python
# Works with OpenAI SDK with minimal changes!
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_GROQ_API_KEY",
    base_url="https://api.groq.com/openai/v1"
)

response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

---

## ğŸ’° **Pricing (January 2025)**

| Model | Input (per 1M tokens) | Output (per 1M tokens) | Speed | Tool Calling |
|-------|----------------------|------------------------|-------|--------------|
| **llama-3.3-70b-versatile** | $0.59 | $0.79 | Ultra-fast | âœ… Yes |
| **llama-3.1-8b-instant** | $0.05 | $0.08 | Blazing! | âœ… Yes |
| **llama-3.1-70b-versatile** | $0.59 | $0.79 | Fast | âœ… Yes |
| **mixtral-8x7b** | $0.24 | $0.24 | Fast | âœ… Yes |
| **gemma2-9b-it** | $0.20 | $0.20 | Fast | âœ… Yes |

### **With Prompt Caching (Automatic)**
- **Cached Input**: 50% discount (e.g., $0.59 â†’ $0.295)
- **Output**: No discount ($0.79 remains $0.79)

---

## âš¡ **Prompt Caching**

### **How It Works**
- **Automatic**: No code changes required!
- **Prefix Matching**: Caches identical prompt prefixes
- **Duration**: Few hours (auto-expires)
- **Discount**: 50% on cached input tokens
- **Not Guaranteed**: Depends on internal routing

### **Example**
```
Request 1: "System prompt + tools + user query"
â†’ Creates cache, full price

Request 2: "System prompt + tools + different query"  
â†’ Cache hit on "System prompt + tools"
â†’ Only "different query" charged full price
â†’ 50% savings on cached portion
```

### **Best Practices**
1. Put **static content first** (system prompt, tools, examples)
2. Put **dynamic content last** (user query, context)
3. Keep prompts structured consistently

---

## ğŸ› ï¸ **Tool Calling (Function Calling)**

### **Status: âœ… SUPPORTED**

Models with tool calling:
- llama-3.3-70b-versatile
- llama-3.1-70b-versatile
- llama-3.1-8b-instant
- mixtral-8x7b

### **Format: OpenAI Compatible**
```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City name"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[{"role": "user", "content": "What's the weather in SF?"}],
    tools=tools,
    tool_choice="auto"
)
```

### **Response Format**
```python
# Tool call response
{
    "choices": [{
        "message": {
            "role": "assistant",
            "tool_calls": [{
                "id": "call_123",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": '{"location": "San Francisco"}'
                }
            }]
        }
    }]
}
```

---

## ğŸŒŠ **Streaming**

### **âœ… Fully Supported**
```python
stream = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

### **Streaming with Tools**
```python
# Same as OpenAI - streams tool calls incrementally
for chunk in stream:
    delta = chunk.choices[0].delta
    if delta.tool_calls:
        print(f"Tool: {delta.tool_calls[0].function.name}")
```

---

## ğŸ”‘ **Authentication**

```bash
# Get API key from: https://console.groq.com/keys
export GROQ_API_KEY="gsk_..."
```

```python
# In code
client = OpenAI(
    api_key=os.environ.get("GROQ_API_KEY"),
    base_url="https://api.groq.com/openai/v1"
)
```

---

## ğŸ“Š **Response Format**

### **Standard Response**
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "llama-3.3-70b-versatile",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Hello! How can I help?"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 8,
    "total_tokens": 18,
    "prompt_tokens_details": {
      "cached_tokens": 0  // Groq-specific: cache hit info
    }
  }
}
```

---

## âš–ï¸ **Groq vs Anthropic (Claude)**

| Feature | Anthropic Claude | Groq |
|---------|-----------------|------|
| **Price (70B)** | $3/$15 per 1M tokens | $0.59/$0.79 per 1M tokens |
| **Speed** | Fast | **Ultra-fast** (10x faster) |
| **Cache Discount** | **90%** ($3 â†’ $0.30) | 50% ($0.59 â†’ $0.295) |
| **Cache Control** | âœ… Explicit (`cache_control`) | âš ï¸ Automatic (no control) |
| **Cache Guarantee** | âœ… Yes (same API key) | âš ï¸ No (routing-dependent) |
| **Cache Duration** | 5 min â†’ 1 hour (extended) | Few hours (unknown) |
| **Cache Shared** | âœ… Org-wide | âš ï¸ Per-instance |
| **Tool Calling** | âœ… Excellent (>95% reliability) | âœ… Good |
| **Reasoning** | âœ… Excellent (complex tasks) | âš ï¸ Good (simpler tasks) |
| **Context Window** | 200K tokens | 128K tokens |

---

## ğŸ¯ **When to Use Groq**

### **âœ… GREAT FOR:**
- Simple queries (navigation, listing)
- High-volume, low-complexity workloads
- Budget-conscious applications
- Speed-critical applications
- Tasks without complex reasoning

### **âŒ NOT IDEAL FOR:**
- Complex multi-step reasoning
- Critical tool calling reliability
- Tasks requiring guaranteed caching
- Long context tasks (>128K tokens)

---

## ğŸ’¡ **Hybrid Strategy**

### **Route by Complexity**

```python
def route_to_model(query: str, has_tools: bool) -> str:
    """Intelligent LLM routing"""
    
    # Complex = use Claude
    if has_tools or len(query) > 200 or any(kw in query.lower() for kw in 
        ["plan", "schedule", "multiple", "restore", "revert"]):
        return "claude-sonnet-4"  # $3/$15, 90% cache
    
    # Simple = use Groq
    else:
        return "llama-3.3-70b"  # $0.59/$0.79, 50% cache
```

### **Cost Comparison**

**100 queries (50 simple, 50 complex):**

**All Claude:**
- Cost: 100 Ã— $0.015 = $1.50
- Quality: Excellent

**Hybrid (Groq for simple, Claude for complex):**
- Simple: 50 Ã— $0.002 = $0.10 (Groq)
- Complex: 50 Ã— $0.015 = $0.75 (Claude)
- **Total: $0.85** (43% cheaper!)
- Quality: Excellent where it matters

---

## ğŸ”§ **Migration from Claude to Groq**

### **Minimal Changes Needed**

```python
# BEFORE (Anthropic)
import anthropic
client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
response = client.messages.create(
    model="claude-sonnet-4",
    max_tokens=1024,
    system="You are helpful",
    messages=[{"role": "user", "content": "Hello"}]
)

# AFTER (Groq with OpenAI SDK)
from openai import OpenAI
client = OpenAI(
    api_key=os.getenv("GROQ_API_KEY"),
    base_url="https://api.groq.com/openai/v1"
)
response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    max_tokens=1024,
    messages=[
        {"role": "system", "content": "You are helpful"},
        {"role": "user", "content": "Hello"}
    ]
)
```

### **Key Differences:**
1. **System message**: Separate in Anthropic, in messages array for Groq/OpenAI
2. **SDK**: `anthropic` â†’ `openai`
3. **Method**: `messages.create` â†’ `chat.completions.create`
4. **Response**: `.content[0].text` â†’ `.choices[0].message.content`

---

## ğŸ“š **Resources**

- **Docs**: https://console.groq.com/docs
- **Pricing**: https://groq.com/pricing
- **Caching**: https://console.groq.com/docs/prompt-caching
- **API Keys**: https://console.groq.com/keys
- **Examples**: https://github.com/groq/groq-api-cookbook

---

## âœ… **Summary**

**Groq is GREAT for:**
- ğŸš€ **Speed** (10x faster than Claude)
- ğŸ’° **Cost** (5x cheaper than Claude)
- ğŸ”§ **Easy migration** (OpenAI-compatible)

**Claude is BETTER for:**
- ğŸ§  **Complex reasoning**
- ğŸ¯ **Guaranteed caching** (90% savings)
- ğŸ› ï¸ **Tool calling reliability** (>95%)
- ğŸ“ **Long context** (200K tokens)

**RECOMMENDATION: Use both!**
- Groq for simple queries â†’ Save 90% on those
- Claude for complex tasks â†’ Get best quality where it matters
- Combined: Save 40-50% overall with zero quality loss

